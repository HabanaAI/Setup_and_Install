apiVersion: kubeflow.org/v2beta1
kind: MPIJob
metadata:
  name: template-metadata-name
  namespace: default
  labels:
    app: hhs-hccl
spec:
  slotsPerWorker: 8
  runPolicy:
    cleanPodPolicy: Running
  mpiReplicaSpecs:
    Launcher:
      replicas: 1
      template:
        metadata:
          labels:
            app: hhs-hccl
        spec:
          volumes:
            - name: mydir
              hostPath:
                path: template-volume-mydir
                type: Directory
          containers:
            - image: template-container-image
              name: pt-hhs-launcher
              workingDir: /habana_health_screen
              volumeMounts:
                - name: mydir
                  mountPath: /habana_health_screen
              env:
                - name: JOB_ID
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.labels['name']
                - name: MY_NODE_NAME
                  valueFrom:
                    fieldRef:
                      fieldPath: spec.nodeName
                - name: HOME_DIR
                  value: "/habana_health_screen"
                - name: HHS_LEVEL
                  value: "2"
              command: ["/bin/bash", "-c"]
              args:
                - >-
                  declare -xr HOSTSFILE=$OMPI_MCA_orte_default_hostfile;

                  declare -xr NUM_NODES=$(wc -l < $HOSTSFILE);
                  declare -xr NGPU_PER_NODE=8;
                  declare -xr N_CARDS=$((NUM_NODES*NGPU_PER_NODE));

                  cd ${HOME_DIR}/build/hccl_demo;
                  declare -xr CMD="python ${HOME_DIR}/build/hccl_demo/run_hccl_demo.py \
                    --test all_reduce \
                    --loop 1000 \
                    --size 32m \
                    -mpi ";

                  set -eo pipefail;

                  mkdir -p $HOME_DIR/$LOG_DIR/L2/$ROUND/;
                  cat /dev/null > $HOME_DIR/$LOG_DIR/L2/$ROUND/$JOB_ID.log;
                  touch $HOME_DIR/$LOG_DIR/L2/$ROUND/$JOB_ID.log;
                  echo "Target Nodes: $TARGET_NODES" > $HOME_DIR/$LOG_DIR/L2/$ROUND/$JOB_ID.log;

                  $CMD \
                    -np ${N_CARDS} \
                    --allow-run-as-root \
                    --bind-to core \
                    --map-by ppr:4:socket:PE=6 \
                    --rank-by core --report-bindings \
                    --tag-output \
                    --merge-stderr-to-stdout --prefix $MPI_ROOT \
                    -x PYTHONPATH="/usr/lib/habanalabs/:$PYTHONPATH" \
                    -x ENABLE_CONSOLE="true" -x LOG_LEVEL_ALL=4 \
                    -x MAX_TIMEOUT=60 2>&1 | tee -a $HOME_DIR/$LOG_DIR/L2/$ROUND/$JOB_ID.log;

                  cd ${HOME_DIR};
                  python ${HOME_DIR}/screen.py --hhs-check hccl-demo --target-nodes $TARGET_NODES --job-id $JOB_ID --logs-dir $LOG_DIR --round $ROUND;

    Worker:
      replicas: template-num-nodes
      template:
        metadata:
          labels:
            app: hhs-hccl
        spec:
          affinity:
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                  - matchExpressions:
                      - key: kubernetes.io/hostname
                        operator: In
                        values:
                          - HHS-DUMMY-VAL
          volumes:
            - name: mydir
              hostPath:
                path: template-volume-mydir
                type: Directory
          tolerations:
            - key: ""
              operator: "Exists"
              effect: "NoSchedule"
            - key: ""
              operator: "Exists"
              effect: "NoExecute"
          containers:
            - image: template-container-image
              name: pt-hhs-worker
              resources:
                limits:
                  habana.ai/gaudi: 8
                requests:
                  habana.ai/gaudi: 8
              volumeMounts:
                - name: mydir
                  mountPath: /habana_health_screen
              env:
                - name: HHS_LEVEL
                  value: "2"
                - name: MY_POD_IP
                  valueFrom:
                    fieldRef:
                      fieldPath: status.podIP
                - name: MY_NODE_NAME
                  valueFrom:
                    fieldRef:
                      fieldPath: spec.nodeName
                - name: MY_POD_NAMESPACE
                  valueFrom:
                    fieldRef:
                      fieldPath: metadata.namespace
              command: ["/bin/bash", "-c"]
              args:
                - >-
                  printenv | grep "MY" >> /etc/environment;
                  service ssh start;
                  sleep 365d;
